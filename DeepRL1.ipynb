{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Learning pseudo code\r\n",
    "### 1. Q Value의 파라미터 초기화\r\n",
    "### 2. Agent의 State(s) 초기화\r\n",
    "### 3. 에피소드가 끝날 때까지 다음 (A~E) 반복\r\n",
    "####     A.Q Value에 대한 E-greedy를 이용한 Action(a) 선택(Select_Action)\r\n",
    "####     B.a를 실행하여 r(Reward)과 s'을 관측\r\n",
    "####     C.s'에서 Q Value에 대한 greedy를 이용하여 a'선택(TD-Target)\r\n",
    "####     D.Value 업데이트(미분은 파이토치, 텐서플로우와 같은 라이브러리가 대신 해주므로 손실 함수는 설정)\r\n",
    "####     E.s<-s'\r\n",
    "### 4. 에피소드가 끝나면 다시 2번으로 돌아가서 Value가 수렴할 때까진 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\r\n",
    "import collections #리플레이 버퍼 - 데이터의 사용자가 지정한 갯수만큼 최신 데이터로만 보유해주는 라이브러리(선입선출)\r\n",
    "import random\r\n",
    "\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#하이퍼 파라미터\r\n",
    "learning_rate = 0.0005\r\n",
    "gamma = 0.98\r\n",
    "buffer_limit = 50000 #CartPole은 비교적 단순한 문제로 한도를 5만으로 감소\r\n",
    "batch_size = 32 #하나의 mini_batch 안에 32개의 데이터가 쓰임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#리플레이 버퍼 Class\r\n",
    "#최신 5만 개의 데이터를 들고 있다가 필요한 때마다 batch_size 만큼의 데이터를 뽑아서 제공해주는 것\r\n",
    "class ReplayBuffer(): \r\n",
    "    def __init__(self):\r\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\r\n",
    "\r\n",
    "    #데터를 버퍼에 넣어주는 함수\r\n",
    "    def put(self, transition): \r\n",
    "        self.buffer.append(transition)\r\n",
    "\r\n",
    "    #랜덤하게 32개의 데이터를 뽑아 미니배치를 구성해주는 함수\r\n",
    "    #done_mask는 종료 상태의 value를 마스킹해주기 위한 변수(종료 상태의 Value를 0으로 만들어 줌)\r\n",
    "    #32개의 데이터는 각기 종류에 맞게 (s는 s끼리) 요소별로 모아서 pytorch의 텐서로 변환해주는 작업을 진행\r\n",
    "    def sample(self, n):\r\n",
    "        mini_batch = random.sample(self.buffer, n)\r\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\r\n",
    "\r\n",
    "        for transition in mini_batch:\r\n",
    "            s, a, r, s_prime, done_mask = transition\r\n",
    "            s_lst.append(s)\r\n",
    "            a_lst.append([a])\r\n",
    "            r_lst.append([r])\r\n",
    "            s_prime_lst.append(s_prime)\r\n",
    "            done_mask_lst.append([done_mask])\r\n",
    "\r\n",
    "        return torch.tensor(s_lst, dtype=torch. float), \r\n",
    "        torch.tensor(a_lst), torch.tensor(r_lst), \r\n",
    "        torch.tensor(s_prime_lst, dtype=torch. float), \r\n",
    "        torch.tensor(done_mask_lst)\r\n",
    "\r\n",
    "    def size(self):\r\n",
    "        return len(self.buffer)                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q_value network class\r\n",
    "#nn.module 클래스를 상속받아 선언됌 - 뉴럴넷을 만들때 뼈대가 되는 클래스\r\n",
    "#Q 네트워크의 파라미터들을 타킷 네트워크로 복사할 때 load_state_dict() 함수를 사용해 한 줄만으로 구현이 가능\r\n",
    "class Qnet(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(Qnet, self).__init__()\r\n",
    "        self.fc1 = nn.Linear(4, 128)\r\n",
    "        self.fc2 = nn.Linear(128, 128)\r\n",
    "        self.fc3 = nn.Linear(128, 2)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = F.relu(self.fc1(x))\r\n",
    "        x = F.relu(self.fc2(x))\r\n",
    "        x = self.fc3(x)\r\n",
    "        return x\r\n",
    "\r\n",
    "    def sample_action(self, obs, epsilon):\r\n",
    "        out = self.forward(obs)\r\n",
    "        coin = random.random()\r\n",
    "        if coin < epsilon:\r\n",
    "            return random.randint(0, 1)\r\n",
    "        else:\r\n",
    "            return out.argmax().item() #Q값이 제일 큰 액션을 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습 함수 - 에피소드 하나가 끝날 때마다 호출\r\n",
    "def train(q, q_target, memory, optimizer):\r\n",
    "    for i in range(10): #한 번 호출할 때마다 10개의 미니 배치를 뽑아 총 10번 업데이트하도록 구현 (한 에피소드에 총 320개의 데이터를 뽑아 사용)\r\n",
    "        s, a, r, s_prime, done_mask = memory.sample(batch_size)\r\n",
    "\r\n",
    "        q_out = q(s)\r\n",
    "        q_a = q_out.gather(1,a)\r\n",
    "        max_q_prime = q_target(s_prime). max (1)[0].unsqueeze(1)\r\n",
    "        target = r + gamma * max_q_prime * done_mask\r\n",
    "        loss = F.smooth_l1_loss(q_a, target)\r\n",
    "\r\n",
    "        optimizer.zero_grad()\r\n",
    "        loss.backword() #역전파 \r\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main 함수\r\n",
    "def main():\r\n",
    "    env = gym.make('CartPole-v1')\r\n",
    "    q = Qnet()\r\n",
    "    q_target = Qnet()\r\n",
    "    q_target.load_state_dict(q.state_dict()) #q 네트워크의 파라미터의 값들을 q_target으로 전부 복사\r\n",
    "    memory = ReplayBuffer()\r\n",
    "\r\n",
    "    print_interval = 20\r\n",
    "    score = 0.0\r\n",
    "    optimizer = optim.Adam(q.parameters(), lr=learning_rate)\r\n",
    "\r\n",
    "    for n_epi in range(10000): #에피소드 진행 수\r\n",
    "        epsilon = max(0.01, 0.08 - 0.01*(n_epi/200))\r\n",
    "        #Decaying epsilon\r\n",
    "        s = env.reset()\r\n",
    "        done = False\r\n",
    "\r\n",
    "        while not done:\r\n",
    "            a = q.sample_action(torch.from_numpy(s). float (), epsilon)\r\n",
    "            s_prime, r, done, info = env.step(a)\r\n",
    "            done_mask = 0.0 if done else 1.0\r\n",
    "            memory.put((s,a,r/100.0, s_prime, done_mask)) #보상을 적절하게 주기 위해서 100으로 나누어 줌\r\n",
    "            s = s_prime\r\n",
    "            score += r\r\n",
    "            if done:\r\n",
    "                break\r\n",
    "        \r\n",
    "        if memory.size()>2000: #데이터가 많이 쌓이지 않은 초기 상태에서 학습을 진행 시 학습이 치우치는 걸 방지하기 위해 2000개를 넘었을 때부터 학습 진행\r\n",
    "            train(q, q_target, memory, optimizer)\r\n",
    "\r\n",
    "        if n_epi%print_interval==0 and n_epi != 0:\r\n",
    "            q_target.load_state_dict(q.state_dict())\r\n",
    "            print(\"n_episode : {}, score : {:.1f}, n_butter : {}, eps : {:.1f}%\".format (n_epi, score/print_interval, memory.size(), epsilon*100))\r\n",
    "            score = 0.0\r\n",
    "\r\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode : 20, score : 14.1, n_butter : 282, eps : 7.9%\n",
      "n_episode : 40, score : 13.4, n_butter : 550, eps : 7.8%\n",
      "n_episode : 60, score : 14.0, n_butter : 830, eps : 7.7%\n",
      "n_episode : 80, score : 13.5, n_butter : 1100, eps : 7.6%\n",
      "n_episode : 100, score : 13.3, n_butter : 1366, eps : 7.5%\n",
      "n_episode : 120, score : 13.4, n_butter : 1634, eps : 7.4%\n",
      "n_episode : 140, score : 13.3, n_butter : 1901, eps : 7.3%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-39-063495f37d39>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_epi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mprint_interval\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mn_epi\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-41f86ece920a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(q, q_target, memory, optimizer)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms_prime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mq_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 1)"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "183bbf6827d058c2a2fb0f4acdc0420849dda2b4380af0e437e38c64d798d8b7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}