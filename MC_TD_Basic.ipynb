{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo 학습 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid world class\r\n",
    "class GridWorld():\r\n",
    "    def __init__(self):\r\n",
    "        self.x=0\r\n",
    "        self.y=0\r\n",
    "    \r\n",
    "    #Action\r\n",
    "    def step(self, a): #Agent로부터 Action을 받아서 상태 변이를 일으키고 보상을 정해주는 함수\r\n",
    "        if a == 0:\r\n",
    "            self.move_right()\r\n",
    "        elif a == 1:\r\n",
    "            self.move_left()\r\n",
    "        elif a == 2:\r\n",
    "            self.move_up()\r\n",
    "        elif a == 3:\r\n",
    "            self.move_down()\r\n",
    "    \r\n",
    "        reward = -1 #어느 방향으로 가든 무조건 보상 -1\r\n",
    "        done = self.is_done() #에피소드가 끝났는 지 알려주는 함수\r\n",
    "        return (self.x, self.y), reward, done\r\n",
    "\r\n",
    "    def move_right(self):\r\n",
    "        self.y += 1\r\n",
    "        if self.y > 3:\r\n",
    "            self.y = 3\r\n",
    "        \r\n",
    "    def move_left(self):\r\n",
    "        self.y -= 1\r\n",
    "        if self.y < 0:\r\n",
    "            self.y = 0\r\n",
    "    \r\n",
    "    def move_up(self):\r\n",
    "        self.x -= 1\r\n",
    "        if self.x < 0:\r\n",
    "            self.x = 0\r\n",
    "\r\n",
    "    def move_down(self):\r\n",
    "        self.x += 1\r\n",
    "        if self.x > 3:\r\n",
    "            self.x = 3\r\n",
    "\r\n",
    "    def is_done(self):\r\n",
    "        if self.x == 3 and self.y == 3: #(3,3)-도착지점에 도착하면 True 아닐 시 False\r\n",
    "            return True\r\n",
    "        else:\r\n",
    "            return False\r\n",
    "\r\n",
    "    def get_state(self):\r\n",
    "        return(self.x, self.y)\r\n",
    "\r\n",
    "    def reset(self): #도착지점에 도달 시 처음 상태로 돌아가기 위해 위치 reset\r\n",
    "        self.x = 0\r\n",
    "        self.y = 0\r\n",
    "        return(self.x, self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agent Class\r\n",
    "class Agent():\r\n",
    "    def __init__(self):\r\n",
    "        pass\r\n",
    "    \r\n",
    "    def select_action(self): #Agent가 네 방향으로 향하는 확률이 모두 동일하기 때문에 설정이 쉬움(아래 코드 참조)\r\n",
    "        coin = random.random()\r\n",
    "        if coin < 0.25:\r\n",
    "            action = 0\r\n",
    "        elif coin < 0.5:\r\n",
    "            action = 1\r\n",
    "        elif coin < 0.75:\r\n",
    "            action = 2\r\n",
    "        else:\r\n",
    "            action = 3\r\n",
    "        return action\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main 함수\r\n",
    "def main():\r\n",
    "    env = GridWorld() #기존에 구축한 GridWorld를 환경으로 선언\r\n",
    "    agent = Agent() #Agent 선언\r\n",
    "    data = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]] #테이블 초기화\r\n",
    "    gamma = 1.0\r\n",
    "    alpha = 0.0001\r\n",
    "\r\n",
    "    for k in range(50000): # 에피소드 진행 수 (Agent가 경험을 쌓는 과정)\r\n",
    "        done = False\r\n",
    "        history = []\r\n",
    "        while not done:\r\n",
    "            action = agent.select_action()\r\n",
    "            (x,y), reward, done = env.step(action)\r\n",
    "            history.append((x,y,reward))\r\n",
    "        env.reset()\r\n",
    "\r\n",
    "        #매 에피소드가 끝나고 바로 해당 데이터를 이용해 테이블을 업데이트\r\n",
    "        cum_reward = 0 #Return(Gt)\r\n",
    "        for transition in history[::-1]:\r\n",
    "            #방문했던 상태들을 뒤에서부터 보며 차례차례 리턴을 계산\r\n",
    "            x, y, reward = transition\r\n",
    "            data[x][y] = data[x][y] + alpha*(cum_reward-data[x][y])\r\n",
    "            cum_reward = cum_reward + gamma*reward\r\n",
    "\r\n",
    "    #학습이 끝나고 난 후 데이터를 출력해보기 위한 코드\r\n",
    "    for row in data:\r\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-58.960320376619414, -57.637937192441555, -54.179227295487614, -50.43379631413986]\n",
      "[-56.79424504786019, -53.6771386655389, -48.39964974904007, -42.89063837708747]\n",
      "[-52.87814584937751, -47.76176454223333, -37.53512708426639, -25.625137397394273]\n",
      "[-49.386947333782636, -42.51683038907416, -25.65218788502933, 0.0]\n"
     ]
    }
   ],
   "source": [
    "main() #10000번 진행 시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-58.87996734442347, -56.91230004592524, -54.182616839454646, -51.6908488752423]\n",
      "[-56.9307120769497, -53.41797859110891, -48.85198934430854, -44.904140800672266]\n",
      "[-53.317856297612515, -48.38795633428589, -39.82394910009695, -30.092740783888104]\n",
      "[-50.95835364909059, -43.78369885020527, -28.86545252039, 0.0]\n"
     ]
    }
   ],
   "source": [
    "main() #에피소드 50000번 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에피소드 진행이 길어질 수록 실제 값에 가까워지는 것을 확인 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference\r\n",
    "### Monte carlo의 학습 방식은 치명적인 단점이 존재한다.\r\n",
    "#### 하나의 Episode가 끝나야지만 업데이트가 이루어진다는 것.\r\n",
    "업데이트를 하기 위해서 Return 값을 필요로 하기 때문인데 즉, \r\n",
    "MC는 적용할 수 있는 환경이 매우 제한적이라는 것이다.\r\n",
    "하지만 현실에서는 뚜렷한 목적이 존재하지 않는 경우가 더 많이 때문에 MC는 적합하다고 볼 수 없는데 이에 대한 해결책이 바로 TD 방법론이다.\r\n",
    "\r\n",
    "TD는 간단하게 설명하면 미래의 추측을 통해 과거의 추측을 변경하는 방법인데 이해하기 어려우니 예시를 들어보자.\r\n",
    "금요일(일요일 강수확률 50%) / 토요일(일요일 강수확률 80%)라고 가정을 해보자\r\n",
    "이렇게 일요일 강수확률이 금요일과 토요일 각각 존재한다고 할 때 당연하게도 토요일이라는 정보가 추가된 토요일의 예측이 더 정확할 것이다.\r\n",
    "이러한 경우처럼 토요일의 추측을 통해 금요일의 추측을 변경하는 것으로 이러한 방법론을 적용하는 경우에는 목표지점을 알지 못 하더라도\r\n",
    "값을 업데이트 하는 것이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporal Difference 학습 구현\r\n",
    "def TD_main():\r\n",
    "    env = GridWorld()\r\n",
    "    agent = Agent()\r\n",
    "    data = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]\r\n",
    "    gamma = 1.0\r\n",
    "    alpha = 0.01 #MC보다 크게 적용\r\n",
    "\r\n",
    "    for k in range(50000):\r\n",
    "        done = False\r\n",
    "        while not done:\r\n",
    "            x, y = env.get_state()\r\n",
    "            action = agent.select_action()\r\n",
    "            (x_prime, y_prime), reward, done = env.step(action)\r\n",
    "\r\n",
    "            #한번의 Step이 진행되자 마자 바로 테이블의 데이터를 업데이트 해줌\r\n",
    "            data[x][y] = data[x][y] + alpha*(reward+gamma*data[x_prime][y_prime]-data[x][y])\r\n",
    "        env.reset()\r\n",
    "\r\n",
    "    for row in data:\r\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-59.289709615001996, -57.59948891273813, -54.01352760770926, -50.38131071503045]\n",
      "[-57.11021750732072, -54.00920503216625, -48.98453392928103, -43.558415925425244]\n",
      "[-54.22446772991143, -48.71689927795067, -38.246364890967584, -26.411818129081418]\n",
      "[-52.21700927997843, -45.14756456121253, -28.894331937603805, 0]\n"
     ]
    }
   ],
   "source": [
    "TD_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC VS TD\r\n",
    "## 1. Episodic MDP \r\n",
    "### 종료 상태가 존재해 Agent의 경험이 Episode 단위로 나뉘어지는 것\r\n",
    "## 2. Non-Episodic MDP\r\n",
    "### 종료 상태 없이 하나의 EPisode가 무한히 이어지는 MDP\r\n",
    "# \r\n",
    "#### MC의 경우에는 1번의 상황에만 적용이 가능하지만 그만큼 Episode 진행이 많아질수록 절대 틀릴 수 없는 방법론(안정적인 방법)\r\n",
    "#### TD는 1번과 2번 어디에나 적용이 가능하다는 장점이 존재!(편향성이 존재하지만 Deep RL에서 어느정도 해결이 가능)\r\n",
    "## 분산의 측면\r\n",
    "## MC - 하나의 Episode가 목적지에 도달하기 까지 수 많은 변수가 존재하기 때문에 분산이 매우 크게 나타남\r\n",
    "## TD - 하나의 Sample만을 참고해 업데이트를 하기 때문에 분산이 매우 작음\r\n",
    "## *어떤 방법론이 더 좋다라고 구분할 수는 없지만 상황에 따라 각기의 존재함.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40b0a2c4feb1a313953209683718f65c38d6b1c241936ad576f53278c4182a1d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}