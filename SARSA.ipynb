{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upgrad Grid World\r\n",
    "## MC 방법론 사용\r\n",
    "#### 한 에피소드의 경험을 쌓고\r\n",
    "#### 경험한 데이터로 q(s,a) 테이블의 값을 업데이트하고 (정책 평가)\r\n",
    "#### 업데이트된 q(s,a) 테이블을 이용하여 e-greedy 정책을 만들고 (정책 개선)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\r\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld():\r\n",
    "    def __init__(self):\r\n",
    "        self.x = 0\r\n",
    "        self.y = 0\r\n",
    "\r\n",
    "    def step(self, a):\r\n",
    "        #Action 0 : left, 1 : up, 2 : right, 3 : down\r\n",
    "        if a == 0:\r\n",
    "            self.move_left()\r\n",
    "        elif a == 1:\r\n",
    "            self.move_up()\r\n",
    "        elif a == 2:\r\n",
    "            self.move_right()\r\n",
    "        elif a == 3:\r\n",
    "            self.move_down()\r\n",
    "        \r\n",
    "        reward = -1\r\n",
    "        done = self.is_done()\r\n",
    "        return (self.x, self.y), reward, done\r\n",
    "\r\n",
    "    def move_left(self):\r\n",
    "        if self.y == 0: #벽에 막히는 경우는 무시하기 위함\r\n",
    "            pass\r\n",
    "        elif self.y == 3 and self.x in [0,1,2]:\r\n",
    "            pass\r\n",
    "        elif self.y == 5  and self.x in [2,3,4]:\r\n",
    "            pass\r\n",
    "        else:\r\n",
    "            self.y -= 1\r\n",
    "        \r\n",
    "    def move_right(self):\r\n",
    "        if self.y == 1 and self.x in [0,1,2]:\r\n",
    "            pass\r\n",
    "        elif self.y == 3 and self.x in [2,3,4]:\r\n",
    "            pass\r\n",
    "        elif self.y == 6:\r\n",
    "            pass\r\n",
    "        else:\r\n",
    "            self.y += 1\r\n",
    "        \r\n",
    "    def move_up(self):\r\n",
    "        if self.x == 0:\r\n",
    "            pass\r\n",
    "        elif self.x == 3 and self.y == 2:\r\n",
    "            pass\r\n",
    "        else:\r\n",
    "            self.x -= 1\r\n",
    "\r\n",
    "    def move_down(self):\r\n",
    "        if self.x == 4:\r\n",
    "            pass\r\n",
    "        elif self.x == 1 and self.y == 4:\r\n",
    "            pass\r\n",
    "        else:\r\n",
    "            self.x += 1\r\n",
    "        \r\n",
    "    def is_done(self):\r\n",
    "        if self.x == 4 and self.y == 6: #목표 지점인 (4,6)에 도달하면 끝\r\n",
    "            return True\r\n",
    "        else:\r\n",
    "            return False\r\n",
    "        \r\n",
    "    def reset(self):\r\n",
    "        self.x = 0\r\n",
    "        self.y = 0\r\n",
    "        return (self.x, self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-Agent Class\r\n",
    "class QAgent():\r\n",
    "    def __init__(self):\r\n",
    "        self.q_table = np.zeros((5,7,4)) #Q value를 저장하기 위한 변수 (모두 0으로 초기화)\r\n",
    "        self.eps = 0.9 #e-greedy 값\r\n",
    "        self.alpha = 0.01 \r\n",
    "\r\n",
    "    def select_action(self, s):\r\n",
    "        x, y = s #state\r\n",
    "        coin = random.random()\r\n",
    "        if coin < self.eps : #coin 값이 eps보다 낮을 경우 Exploration 진행 (random action)\r\n",
    "            action = random.randint(0,3)\r\n",
    "        else:\r\n",
    "            action_val = self.q_table[x,y,:]\r\n",
    "            action = np.argmax(action_val)\r\n",
    "        return action\r\n",
    "    \r\n",
    "    def update_table(self, history):\r\n",
    "        cum_reward = 0\r\n",
    "        for transition in history[::-1]:\r\n",
    "            s, a, r, s_prime = transition\r\n",
    "            x, y = s\r\n",
    "            #MC 방법론을 이용한 업데이트\r\n",
    "            self.q_table[x,y,a] = self.q_table[x,y,a] + self.alpha * (cum_reward - self.q_table[x,y,a])\r\n",
    "            cum_reward = cum_reward + r\r\n",
    "\r\n",
    "    def  anneal_eps(self): #decayin e-greedy를 위해 선형적으로 수치 감소\r\n",
    "        self.eps -= 0.03\r\n",
    "        self.eps = max(self.eps, 0.1)\r\n",
    "\r\n",
    "    def show_table(self): #학습이 각 위치에서 어느 Action의 Q Value가 가장 높았는지 보여주는 함수\r\n",
    "        q_lst = self.q_table.tolist()\r\n",
    "        data = np.zeros((5,7))\r\n",
    "        for row_idx in range(len(q_lst)):\r\n",
    "            row = q_lst[row_idx]\r\n",
    "            for col_idx in range(len(row)):\r\n",
    "                col = row[col_idx]\r\n",
    "                action = np.argmax(col)\r\n",
    "                data[row_idx, col_idx] = action\r\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main 함수\r\n",
    "def main():\r\n",
    "    env = GridWorld()\r\n",
    "    agent = QAgent()\r\n",
    "\r\n",
    "    for n_epi in range(1000):\r\n",
    "        done = False\r\n",
    "        history = []\r\n",
    "\r\n",
    "        s = env.reset()\r\n",
    "        while not done: #한 Episode가 끝날 때 까지\r\n",
    "            a = agent.select_action(s)\r\n",
    "            s_prime, r, done = env.step(a)\r\n",
    "            history.append((s, a, r, s_prime))\r\n",
    "            s = s_prime\r\n",
    "        agent.update_table(history)\r\n",
    "        agent.anneal_eps()\r\n",
    "\r\n",
    "    agent.show_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 2. 0. 2. 2. 3. 0.]\n",
      " [3. 3. 0. 1. 1. 2. 3.]\n",
      " [3. 2. 0. 1. 0. 3. 3.]\n",
      " [2. 3. 2. 1. 0. 3. 3.]\n",
      " [2. 2. 1. 1. 0. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA - TD Control\r\n",
    "## S(State) ▶ A(Action) ▶ R(Reward) ▶ S'(State') ▶ A'(Action')\r\n",
    "### TD는 분산이 작다는 MC 보다 큰 장점을 가지고 있는데 이를 활용하기 위해서는 SARSA에 대해서 이해를 먼저 해야한다.\r\n",
    "### 기댓값 안에 Sample을 무수히 모은 후 정책 함수를 이용해 자유롭게 나아가며 각 step 늘어날 때 그 데이터를 이용해 TD 타깃을 계산\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARSA 구현\r\n",
    "class SARSA():\r\n",
    "    def __init__(self):\r\n",
    "        self.q_table = np.zeros((5,7,4))\r\n",
    "        self.eps = 0.9\r\n",
    "\r\n",
    "    def select_action(self, s):\r\n",
    "        x, y = s\r\n",
    "        coin = random.random()\r\n",
    "        if coin < self.eps:\r\n",
    "            action = random.randint(0,3)\r\n",
    "        else:\r\n",
    "            action_val = self.q_table[x,y,:]\r\n",
    "            action = np.argmax(action_val)\r\n",
    "        return action\r\n",
    "\r\n",
    "    def update_table(self, transition):\r\n",
    "        s, a, r, s_prime = transition\r\n",
    "        x, y = s\r\n",
    "        next_x, next_y = s_prime\r\n",
    "        a_prime = self.select_action(s_prime) #s'에서 선택할 action(실제로 취한 action이 아님)\r\n",
    "        #SARSA 업데이트 식을 이용\r\n",
    "        self.q_table[x,y,a] = self.q_table[x,y,a] + 0.1 * (r + self.q_table[next_x, next_y, a_prime]-self.q_table[x,y,a])\r\n",
    "\r\n",
    "    def anneal_eps(self):\r\n",
    "        self.eps -= 0.03\r\n",
    "        self.eps = max(self.eps, 0.1)\r\n",
    "    \r\n",
    "    def show_table(self):\r\n",
    "        q_lst = self.q_table.tolist()\r\n",
    "        data = np.zeros((5,7))\r\n",
    "        for row_idx in range(len(q_lst)):\r\n",
    "            row = q_lst[row_idx]\r\n",
    "            for col_idx in range(len(row)):\r\n",
    "                col = row[col_idx]\r\n",
    "                action = np.argmax(col)\r\n",
    "                data[row_idx, col_idx] = action\r\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARSA main\r\n",
    "def main_sarsa():\r\n",
    "    env = GridWorld()\r\n",
    "    agent = SARSA()\r\n",
    "\r\n",
    "    for n_epi in range(1000):\r\n",
    "        done = False\r\n",
    "\r\n",
    "        s = env.reset()\r\n",
    "        while not done:\r\n",
    "            a = agent.select_action(s)\r\n",
    "            s_prime, r, done = env.step(a)\r\n",
    "            agent.update_table((s,a,r,s_prime))\r\n",
    "            s = s_prime\r\n",
    "        agent.anneal_eps()\r\n",
    "\r\n",
    "    agent.show_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 3. 0. 2. 2. 3. 3.]\n",
      " [3. 3. 0. 2. 2. 3. 3.]\n",
      " [3. 3. 0. 1. 0. 3. 3.]\n",
      " [3. 2. 2. 1. 0. 3. 3.]\n",
      " [0. 2. 3. 0. 0. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "main_sarsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40b0a2c4feb1a313953209683718f65c38d6b1c241936ad576f53278c4182a1d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}