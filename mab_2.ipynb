{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import collections\r\n",
    "import gym\r\n",
    "#!pip install 'tensorflow==1.15.0'\r\n",
    "import tensorflow as tf\r\n",
    "tf.__version__\r\n",
    "import tqdm\r\n",
    "\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "from tensorflow.keras import layers\r\n",
    "from typing import Any, List, Sequence, Tuple\r\n",
    "\r\n",
    "from kaggle_environments import make, evaluate\r\n",
    "\r\n",
    "from gym import spaces\r\n",
    "\r\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df = pd.read_csv('age_range_video.csv')\r\n",
    "df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Campaign ID  Campaign   Ad group ID  Ad group     Age Range  \\\n",
       "0  13539106481  video_ad  120983605102  adgroup1         25-34   \n",
       "1  13539106481  video_ad  120983605102  adgroup1         35-44   \n",
       "2  13539106481  video_ad  120983605102  adgroup1  Undetermined   \n",
       "3  13539106481  video_ad  120983605102  adgroup1         45-54   \n",
       "4  13539106481  video_ad  120983605102  adgroup1         55-64   \n",
       "5  13539106481  video_ad  120983605102  adgroup1    65 or more   \n",
       "6  13539106481  video_ad  120983605102  adgroup1         18-24   \n",
       "\n",
       "  Video played to 25% Video played to 50% Video played to 75%  \\\n",
       "0              33.33%              21.57%              19.61%   \n",
       "1              30.95%              20.24%              14.29%   \n",
       "2               7.61%               4.35%               3.26%   \n",
       "3              19.23%              12.50%               8.65%   \n",
       "4              22.67%              15.33%              14.67%   \n",
       "5              14.78%              10.43%               8.70%   \n",
       "6              21.57%              15.69%              15.69%   \n",
       "\n",
       "  Video played to 100%  Views  Impressions View rate  \n",
       "0               17.65%     20           52    38.46%  \n",
       "1               13.10%     29           86    33.72%  \n",
       "2                2.17%     16           92    17.39%  \n",
       "3                7.69%     30          105    28.57%  \n",
       "4               11.33%     46          150    30.67%  \n",
       "5                6.96%     28          119    23.53%  \n",
       "6                9.80%     19           51    37.25%  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Campaign ID</th>\n",
       "      <th>Campaign</th>\n",
       "      <th>Ad group ID</th>\n",
       "      <th>Ad group</th>\n",
       "      <th>Age Range</th>\n",
       "      <th>Video played to 25%</th>\n",
       "      <th>Video played to 50%</th>\n",
       "      <th>Video played to 75%</th>\n",
       "      <th>Video played to 100%</th>\n",
       "      <th>Views</th>\n",
       "      <th>Impressions</th>\n",
       "      <th>View rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13539106481</td>\n",
       "      <td>video_ad</td>\n",
       "      <td>120983605102</td>\n",
       "      <td>adgroup1</td>\n",
       "      <td>25-34</td>\n",
       "      <td>33.33%</td>\n",
       "      <td>21.57%</td>\n",
       "      <td>19.61%</td>\n",
       "      <td>17.65%</td>\n",
       "      <td>20</td>\n",
       "      <td>52</td>\n",
       "      <td>38.46%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13539106481</td>\n",
       "      <td>video_ad</td>\n",
       "      <td>120983605102</td>\n",
       "      <td>adgroup1</td>\n",
       "      <td>35-44</td>\n",
       "      <td>30.95%</td>\n",
       "      <td>20.24%</td>\n",
       "      <td>14.29%</td>\n",
       "      <td>13.10%</td>\n",
       "      <td>29</td>\n",
       "      <td>86</td>\n",
       "      <td>33.72%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13539106481</td>\n",
       "      <td>video_ad</td>\n",
       "      <td>120983605102</td>\n",
       "      <td>adgroup1</td>\n",
       "      <td>Undetermined</td>\n",
       "      <td>7.61%</td>\n",
       "      <td>4.35%</td>\n",
       "      <td>3.26%</td>\n",
       "      <td>2.17%</td>\n",
       "      <td>16</td>\n",
       "      <td>92</td>\n",
       "      <td>17.39%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13539106481</td>\n",
       "      <td>video_ad</td>\n",
       "      <td>120983605102</td>\n",
       "      <td>adgroup1</td>\n",
       "      <td>45-54</td>\n",
       "      <td>19.23%</td>\n",
       "      <td>12.50%</td>\n",
       "      <td>8.65%</td>\n",
       "      <td>7.69%</td>\n",
       "      <td>30</td>\n",
       "      <td>105</td>\n",
       "      <td>28.57%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13539106481</td>\n",
       "      <td>video_ad</td>\n",
       "      <td>120983605102</td>\n",
       "      <td>adgroup1</td>\n",
       "      <td>55-64</td>\n",
       "      <td>22.67%</td>\n",
       "      <td>15.33%</td>\n",
       "      <td>14.67%</td>\n",
       "      <td>11.33%</td>\n",
       "      <td>46</td>\n",
       "      <td>150</td>\n",
       "      <td>30.67%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13539106481</td>\n",
       "      <td>video_ad</td>\n",
       "      <td>120983605102</td>\n",
       "      <td>adgroup1</td>\n",
       "      <td>65 or more</td>\n",
       "      <td>14.78%</td>\n",
       "      <td>10.43%</td>\n",
       "      <td>8.70%</td>\n",
       "      <td>6.96%</td>\n",
       "      <td>28</td>\n",
       "      <td>119</td>\n",
       "      <td>23.53%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13539106481</td>\n",
       "      <td>video_ad</td>\n",
       "      <td>120983605102</td>\n",
       "      <td>adgroup1</td>\n",
       "      <td>18-24</td>\n",
       "      <td>21.57%</td>\n",
       "      <td>15.69%</td>\n",
       "      <td>15.69%</td>\n",
       "      <td>9.80%</td>\n",
       "      <td>19</td>\n",
       "      <td>51</td>\n",
       "      <td>37.25%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df.sort_values('View rate', inplace=True)\r\n",
    "df.reset_index(inplace=True, drop=True)\r\n",
    "df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Campaign ID  Campaign   Ad group ID  Ad group     Age Range  \\\n",
       "0  13539106481  video_ad  120983605102  adgroup1  Undetermined   \n",
       "1  13539106481  video_ad  120983605102  adgroup1    65 or more   \n",
       "2  13539106481  video_ad  120983605102  adgroup1         45-54   \n",
       "3  13539106481  video_ad  120983605102  adgroup1         55-64   \n",
       "4  13539106481  video_ad  120983605102  adgroup1         35-44   \n",
       "5  13539106481  video_ad  120983605102  adgroup1         18-24   \n",
       "6  13539106481  video_ad  120983605102  adgroup1         25-34   \n",
       "\n",
       "  Video played to 25% Video played to 50% Video played to 75%  \\\n",
       "0               7.61%               4.35%               3.26%   \n",
       "1              14.78%              10.43%               8.70%   \n",
       "2              19.23%              12.50%               8.65%   \n",
       "3              22.67%              15.33%              14.67%   \n",
       "4              30.95%              20.24%              14.29%   \n",
       "5              21.57%              15.69%              15.69%   \n",
       "6              33.33%              21.57%              19.61%   \n",
       "\n",
       "  Video played to 100%  Views  Impressions View rate  \n",
       "0                2.17%     16           92    17.39%  \n",
       "1                6.96%     28          119    23.53%  \n",
       "2                7.69%     30          105    28.57%  \n",
       "3               11.33%     46          150    30.67%  \n",
       "4               13.10%     29           86    33.72%  \n",
       "5                9.80%     19           51    37.25%  \n",
       "6               17.65%     20           52    38.46%  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Campaign ID</th>\n",
       "      <th>Campaign</th>\n",
       "      <th>Ad group ID</th>\n",
       "      <th>Ad group</th>\n",
       "      <th>Age Range</th>\n",
       "      <th>Video played to 25%</th>\n",
       "      <th>Video played to 50%</th>\n",
       "      <th>Video played to 75%</th>\n",
       "      <th>Video played to 100%</th>\n",
       "      <th>Views</th>\n",
       "      <th>Impressions</th>\n",
       "      <th>View rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13539106481</td>\n",
       "      <td>video_ad</td>\n",
       "      <td>120983605102</td>\n",
       "      <td>adgroup1</td>\n",
       "      <td>Undetermined</td>\n",
       "      <td>7.61%</td>\n",
       "      <td>4.35%</td>\n",
       "      <td>3.26%</td>\n",
       "      <td>2.17%</td>\n",
       "      <td>16</td>\n",
       "      <td>92</td>\n",
       "      <td>17.39%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13539106481</td>\n",
       "      <td>video_ad</td>\n",
       "      <td>120983605102</td>\n",
       "      <td>adgroup1</td>\n",
       "      <td>65 or more</td>\n",
       "      <td>14.78%</td>\n",
       "      <td>10.43%</td>\n",
       "      <td>8.70%</td>\n",
       "      <td>6.96%</td>\n",
       "      <td>28</td>\n",
       "      <td>119</td>\n",
       "      <td>23.53%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13539106481</td>\n",
       "      <td>video_ad</td>\n",
       "      <td>120983605102</td>\n",
       "      <td>adgroup1</td>\n",
       "      <td>45-54</td>\n",
       "      <td>19.23%</td>\n",
       "      <td>12.50%</td>\n",
       "      <td>8.65%</td>\n",
       "      <td>7.69%</td>\n",
       "      <td>30</td>\n",
       "      <td>105</td>\n",
       "      <td>28.57%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13539106481</td>\n",
       "      <td>video_ad</td>\n",
       "      <td>120983605102</td>\n",
       "      <td>adgroup1</td>\n",
       "      <td>55-64</td>\n",
       "      <td>22.67%</td>\n",
       "      <td>15.33%</td>\n",
       "      <td>14.67%</td>\n",
       "      <td>11.33%</td>\n",
       "      <td>46</td>\n",
       "      <td>150</td>\n",
       "      <td>30.67%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13539106481</td>\n",
       "      <td>video_ad</td>\n",
       "      <td>120983605102</td>\n",
       "      <td>adgroup1</td>\n",
       "      <td>35-44</td>\n",
       "      <td>30.95%</td>\n",
       "      <td>20.24%</td>\n",
       "      <td>14.29%</td>\n",
       "      <td>13.10%</td>\n",
       "      <td>29</td>\n",
       "      <td>86</td>\n",
       "      <td>33.72%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13539106481</td>\n",
       "      <td>video_ad</td>\n",
       "      <td>120983605102</td>\n",
       "      <td>adgroup1</td>\n",
       "      <td>18-24</td>\n",
       "      <td>21.57%</td>\n",
       "      <td>15.69%</td>\n",
       "      <td>15.69%</td>\n",
       "      <td>9.80%</td>\n",
       "      <td>19</td>\n",
       "      <td>51</td>\n",
       "      <td>37.25%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13539106481</td>\n",
       "      <td>video_ad</td>\n",
       "      <td>120983605102</td>\n",
       "      <td>adgroup1</td>\n",
       "      <td>25-34</td>\n",
       "      <td>33.33%</td>\n",
       "      <td>21.57%</td>\n",
       "      <td>19.61%</td>\n",
       "      <td>17.65%</td>\n",
       "      <td>20</td>\n",
       "      <td>52</td>\n",
       "      <td>38.46%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class MABanditGym:   \r\n",
    "    \r\n",
    "    \r\n",
    "    def __init__(self, agent2=\"random\"):\r\n",
    "        ks_env = make(\"mab\", debug=True)\r\n",
    "        self.env = ks_env.train([None, agent2])\r\n",
    "        self.nrounds = 2000\r\n",
    "        self.banditCount = ks_env.configuration.banditCount\r\n",
    "        self.prev_reward = 0\r\n",
    "        \r\n",
    "        # Learn about spaces here: http://gym.openai.com/docs/#spaces\r\n",
    "        self.action_space = spaces.Discrete(self.banditCount)\r\n",
    "        low = -np.ones((self.nrounds,), dtype=np.float32)\r\n",
    "        high = -low*(self.banditCount-1)\r\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\r\n",
    "        # Tuple corresponding to the min and max possible rewards\r\n",
    "        self.reward_range = (-10,1)\r\n",
    "        self.grid = -np.ones((self.nrounds,2))\r\n",
    "        self.obs=np.array(self.grid).reshape(self.nrounds*2)\r\n",
    "        # StableBaselines throws error if these are not defined\r\n",
    "        self.spec = None\r\n",
    "        self.metadata = None\r\n",
    "    def reset(self):\r\n",
    "        #print(env.obs)\r\n",
    "        self.env.reset()\r\n",
    "        self.grid = -np.ones((self.nrounds,2))\r\n",
    "        self.obs=np.array(self.grid).reshape(self.nrounds*2)\r\n",
    "        self.prev_reward = 0\r\n",
    "        return self.obs\r\n",
    "    def change_reward(self, old_reward, done):\r\n",
    "        if old_reward == 1000: # The agent won the game\r\n",
    "            return 0\r\n",
    "        elif done: # The opponent won the game\r\n",
    "            return -10\r\n",
    "        else: # Reward 1/2000\r\n",
    "            return old_reward\r\n",
    "    def step(self, action):\r\n",
    "        _={}\r\n",
    "        # Check if agent's move is valid\r\n",
    "        is_valid = (int(action) in range(0,self.banditCount) )\r\n",
    "        #valid_moves = [bnd for bnd in range(config.banditCount)]\r\n",
    "       \r\n",
    "        if is_valid: # Play the \"move\"\r\n",
    "            current_obs = self.env.step(int(action))\r\n",
    "            \r\n",
    "            for pos in range(0,2):\r\n",
    "                #print(current_obs)\r\n",
    "                self.grid[current_obs[0]['step']-1][pos]=current_obs[0]['lastActions'][pos]\r\n",
    "            self.obs=np.array(self.grid).reshape(self.nrounds*2)\r\n",
    "            old_reward= current_obs[0]['reward']\r\n",
    "            done = (current_obs[0]['step']==self.nrounds-1 and current_obs[0]['reward']<600)#current_obs[1]['observation']['reward']\r\n",
    "            reward = old_reward- self.prev_reward  #self.change_reward(old_reward, done)\r\n",
    "            self.prev_reward=old_reward\r\n",
    "        else: # End the game and penalize agent\r\n",
    "            reward, done, _ = -10, True, {}\r\n",
    "        #print(self.obs, reward, done, _  )    \r\n",
    "        return self.obs, reward, done, _    \r\n",
    "\r\n",
    "\r\n",
    "    \r\n",
    "    def seed(self, seed=None):\r\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\r\n",
    "        return [seed]\r\n",
    "\r\n",
    "# Create the environment for training tasks\r\n",
    "env = MABanditGym(agent2=\"random\")\r\n",
    "# Set seed for experiment reproducibility\r\n",
    "seed = 42\r\n",
    "env.seed(seed)\r\n",
    "random.seed(seed)\r\n",
    "tf.random.set_seed(seed)\r\n",
    "np.random.seed(seed)\r\n",
    "\r\n",
    "# Small epsilon value for stabilizing division operations\r\n",
    "eps = np.finfo(np.float32).eps.item()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class ActorCritic(tf.keras.Model):\r\n",
    "  \"\"\"Combined actor-critic network.\"\"\"\r\n",
    "\r\n",
    "  def __init__(\r\n",
    "      self, \r\n",
    "      num_actions: int, \r\n",
    "      num_hidden_units: int):\r\n",
    "    \"\"\"Initialize.\"\"\"\r\n",
    "    super().__init__()\r\n",
    "    self.common = layers.Dense(num_hidden_units, activation=\"relu\")\r\n",
    "    self.actor = layers.Dense(num_actions)\r\n",
    "    self.critic = layers.Dense(1)\r\n",
    "\r\n",
    "  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\r\n",
    "    x = self.common(inputs)\r\n",
    "    return self.actor(x), self.critic(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "num_actions = env.action_space.n  # 100\r\n",
    "print(num_actions)\r\n",
    "num_hidden_units = 128\r\n",
    "\r\n",
    "model = ActorCritic(num_actions, num_hidden_units)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "random.betavariate(1, 5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0009373245870087911"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Wrap OpenAI Gym's `env.step` call as an operation in a TensorFlow function.\r\n",
    "# This would allow it to be included in a callable TensorFlow graph.\r\n",
    "\r\n",
    "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\r\n",
    "  \"\"\"Returns state, reward and done flag given an action.\"\"\"\r\n",
    "\r\n",
    "  state, reward, done, _ = env.step(action)\r\n",
    "  return (state.astype(np.float32), \r\n",
    "          np.array(reward, np.int32), \r\n",
    "          np.array(done, np.int32))\r\n",
    "\r\n",
    "\r\n",
    "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\r\n",
    "  return tf.numpy_function(env_step, [action], \r\n",
    "                           [tf.float32, tf.int32, tf.int32])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def run_episode(\r\n",
    "    initial_state: tf.Tensor,  \r\n",
    "    model: tf.keras.Model, \r\n",
    "    max_steps: int) -> List[tf.Tensor]:\r\n",
    "  \"\"\"Runs a single episode to collect training data.\"\"\"\r\n",
    "\r\n",
    "  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\r\n",
    "  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\r\n",
    "  rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\r\n",
    "\r\n",
    "  initial_state_shape = initial_state.shape\r\n",
    "  state = initial_state\r\n",
    "\r\n",
    "  for t in tf.range(max_steps):\r\n",
    "    # Convert state into a batched tensor (batch size = 1)\r\n",
    "    state = tf.expand_dims(state, 0)\r\n",
    "\r\n",
    "    # Run the model and to get action probabilities and critic value\r\n",
    "    action_logits_t, value = model(state)\r\n",
    "\r\n",
    "    # Sample next action from the action probability distribution\r\n",
    "    action = tf.random.categorical(action_logits_t, 1)[0, 0]\r\n",
    "    action_probs_t = tf.nn.softmax(action_logits_t)\r\n",
    "\r\n",
    "    # Store critic values\r\n",
    "    values = values.write(t, tf.squeeze(value))\r\n",
    "\r\n",
    "    # Store log probability of the action chosen\r\n",
    "    action_probs = action_probs.write(t, action_probs_t[0, action])\r\n",
    "\r\n",
    "    # Apply action to the environment to get next state and reward\r\n",
    "    state, reward, done = tf_env_step(action)\r\n",
    "    print(reward)\r\n",
    "    state.set_shape(initial_state_shape)\r\n",
    "\r\n",
    "     # Store reward\r\n",
    "    rewards = rewards.write(t, reward)\r\n",
    "\r\n",
    "    if tf.cast(done, tf.bool):\r\n",
    "      break\r\n",
    "\r\n",
    "  action_probs = action_probs.stack()\r\n",
    "  values = values.stack()\r\n",
    "  rewards = rewards.stack()\r\n",
    "\r\n",
    "  return action_probs, values, rewards"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def get_expected_return(\r\n",
    "    rewards: tf.Tensor, \r\n",
    "    gamma: float, \r\n",
    "    standardize: bool = True) -> tf.Tensor:\r\n",
    "  \"\"\"Compute expected returns per timestep.\"\"\"\r\n",
    "\r\n",
    "  n = tf.shape(rewards)[0]\r\n",
    "  returns = tf.TensorArray(dtype=tf.float32, size=n)\r\n",
    "\r\n",
    "  # Start from the end of `rewards` and accumulate reward sums\r\n",
    "  # into the `returns` array\r\n",
    "  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\r\n",
    "  discounted_sum = tf.constant(0.0)\r\n",
    "  discounted_sum_shape = discounted_sum.shape\r\n",
    "  for i in tf.range(n):\r\n",
    "    reward = rewards[i]\r\n",
    "    discounted_sum = reward + gamma * discounted_sum\r\n",
    "    discounted_sum.set_shape(discounted_sum_shape)\r\n",
    "    returns = returns.write(i, discounted_sum)\r\n",
    "  returns = returns.stack()[::-1]\r\n",
    "\r\n",
    "  if standardize:\r\n",
    "    returns = ((returns - tf.math.reduce_mean(returns)) / \r\n",
    "               (tf.math.reduce_std(returns) + eps))\r\n",
    "\r\n",
    "  return returns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\r\n",
    "\r\n",
    "def compute_loss(\r\n",
    "    action_probs: tf.Tensor,  \r\n",
    "    values: tf.Tensor,  \r\n",
    "    returns: tf.Tensor) -> tf.Tensor:\r\n",
    "  \"\"\"Computes the combined actor-critic loss.\"\"\"\r\n",
    "\r\n",
    "  advantage = returns - values\r\n",
    "\r\n",
    "  action_log_probs = tf.math.log(action_probs)\r\n",
    "  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\r\n",
    "\r\n",
    "  critic_loss = huber_loss(values, returns)\r\n",
    "\r\n",
    "  return actor_loss + critic_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\r\n",
    "\r\n",
    "\r\n",
    "@tf.function\r\n",
    "def train_step(\r\n",
    "    initial_state: tf.Tensor, \r\n",
    "    model: tf.keras.Model, \r\n",
    "    optimizer: tf.keras.optimizers.Optimizer, \r\n",
    "    gamma: float, \r\n",
    "    max_steps_per_episode: int) -> tf.Tensor:\r\n",
    "  \"\"\"Runs a model training step.\"\"\"\r\n",
    "  #print(\"Hello\")\r\n",
    "  with tf.GradientTape() as tape:\r\n",
    "\r\n",
    "    # Run the model for one episode to collect training data\r\n",
    "    action_probs, values, rewards = run_episode(\r\n",
    "        initial_state, model, max_steps_per_episode) \r\n",
    "\r\n",
    "    # Calculate expected returns\r\n",
    "    returns = get_expected_return(rewards, gamma)\r\n",
    "\r\n",
    "    # Convert training data to appropriate TF tensor shapes\r\n",
    "    action_probs, values, returns = [\r\n",
    "        tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \r\n",
    "\r\n",
    "    # Calculating loss values to update our network\r\n",
    "    loss = compute_loss(action_probs, values, returns)\r\n",
    "\r\n",
    "  # Compute the gradients from the loss\r\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\r\n",
    "\r\n",
    "  # Apply the gradients to the model's parameters\r\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n",
    "\r\n",
    "  episode_reward = tf.math.reduce_sum(rewards)\r\n",
    "\r\n",
    "  return episode_reward"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "%%time\r\n",
    "\r\n",
    "max_episodes = 1\r\n",
    "max_steps_per_episode = 2000\r\n",
    "\r\n",
    "# Problem is considered solved if average reward is >= 600 over 100 \r\n",
    "# consecutive trials\r\n",
    "reward_threshold = 600\r\n",
    "running_reward = 0\r\n",
    "\r\n",
    "# Discount factor for future rewards\r\n",
    "gamma = 0.99\r\n",
    "\r\n",
    "with tqdm.trange(max_episodes) as t:\r\n",
    "  for i in t:\r\n",
    "    new_env=env.reset()\r\n",
    "    #print(new_env)\r\n",
    "    #print(type(new_env))\r\n",
    "    #print(new_env.shape)\r\n",
    "    \r\n",
    "    initial_state = tf.constant(new_env, dtype=tf.float32)\r\n",
    "    episode_reward = int(train_step(\r\n",
    "        initial_state, model, optimizer, gamma, max_steps_per_episode))\r\n",
    "    #print('episode_reward: ', episode_reward)\r\n",
    "    running_reward = episode_reward*0.01 + running_reward*.99\r\n",
    "\r\n",
    "    t.set_description(f'Episode {i}')\r\n",
    "    t.set_postfix(\r\n",
    "        episode_reward=episode_reward, running_reward=running_reward)\r\n",
    "\r\n",
    "    # Show average episode reward every 5 episodes\r\n",
    "    if i % 5 == 0:\r\n",
    "      print(f'Episode {i}: average reward: {running_reward}')\r\n",
    "\r\n",
    "    if running_reward > reward_threshold:  \r\n",
    "        break\r\n",
    "\r\n",
    "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensor(\"while/PyFunc:1\", dtype=int32, device=/job:localhost/replica:0/task:0)\n",
      "Tensor(\"while/PyFunc:1\", dtype=int32, device=/job:localhost/replica:0/task:0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Episode 0: 100%|██████████| 1/1 [00:17<00:00, 17.44s/it, episode_reward=211, running_reward=2.11]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode 0: average reward: 2.11\n",
      "\n",
      "Solved at episode 0: average reward: 2.11!\n",
      "Wall time: 17.4 s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "\r\n",
    "filename = 'bandit_model'\r\n",
    "model.save(filename) # creates a HDF5 file 'bandit_model.h5'\r\n",
    "#del model  # deletes the existing model"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: bandit_model\\assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "from tensorflow.keras.models import load_model\r\n",
    "# returns a compiled model\r\n",
    "# identical to the previous one\r\n",
    "model_bandit = load_model('bandit_model')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "%%writefile agent_random_.py\r\n",
    "import random\r\n",
    "def agent_random_(obs, config):\r\n",
    "    #print(obs)\r\n",
    "    #print(config)\r\n",
    "    valid_moves = [bnd for bnd in range(config['banditCount'])]\r\n",
    "    return random.choice(valid_moves)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting agent_random_.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "%%writefile submission.py\r\n",
    "\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "\r\n",
    "from tensorflow.keras.models import load_model\r\n",
    "# returns a compiled model\r\n",
    "# identical to the previous one\r\n",
    "model_bandit = load_model('bandit_model')\r\n",
    "\r\n",
    "\r\n",
    "class MABanditPlayer:\r\n",
    "    global model_bandit\r\n",
    "    \r\n",
    "    \r\n",
    "    def __init__(self, observation, configuration):\r\n",
    "        \r\n",
    "        self.nrounds = configuration['episodeSteps']\r\n",
    "        self.banditCount = configuration['banditCount']\r\n",
    "        self.prev_reward = 0\r\n",
    "        self.grid = -np.ones((self.nrounds,2))\r\n",
    "        self.obs=np.array(self.grid).reshape(self.nrounds*2)\r\n",
    "        self.prev_reward = 0\r\n",
    "        # StableBaselines throws error if these are not defined\r\n",
    "        \r\n",
    "    def reset(self):\r\n",
    "        #print(env.obs)\r\n",
    "        self.env.reset()\r\n",
    "        self.grid = -np.ones((self.nrounds,2))\r\n",
    "        self.obs=np.array(self.grid).reshape(self.nrounds*2)\r\n",
    "        self.prev_reward = 0\r\n",
    "        return self.obs\r\n",
    "    \r\n",
    "    def play(self, observation, configuration):\r\n",
    "        bandit=0\r\n",
    "        if observation['step']>0:\r\n",
    "            \r\n",
    "        \r\n",
    "            for pos in range(0,2):\r\n",
    "                    #print(current_obs)\r\n",
    "                    self.grid[observation['step']-1][pos]=observation['lastActions'][pos]\r\n",
    "            new_reward= observation['reward']\r\n",
    "            reward = new_reward- self.prev_reward\r\n",
    "            self.prev_reward=new_reward\r\n",
    "            self.obs=np.array(self.grid).reshape(self.nrounds*2)\r\n",
    "            \r\n",
    "            # Convert state into a batched tensor (batch size = 1)\r\n",
    "            state = tf.expand_dims(self.obs,0)\r\n",
    "            # Run the model and to get action probabilities and critic value\r\n",
    "            action_logits_t, value = model_bandit(state)\r\n",
    "            # Sample next action from the action probability distribution\r\n",
    "            action = tf.random.categorical(action_logits_t, 1)[0, 0]\r\n",
    "            #bandit = model_bandit.predict_classes(state) #state = tf.expand_dims(state, 0)\r\n",
    "            with tf.compat.v1.Session() as sess:\r\n",
    "                bandit = action.numpy()\r\n",
    "            \r\n",
    "        else:\r\n",
    "            valid_moves = [bnd for bnd in range(configuration['banditCount'])]\r\n",
    "            bandit = np.dtype('int32').type(random.choice(valid_moves))\r\n",
    "\r\n",
    "        return(bandit)    \r\n",
    "    def seed(self, seed=None):\r\n",
    "        self.np_random, seed = seeding.np_random(seed)\r\n",
    "        return [seed]\r\n",
    "# Create the environment\r\n",
    "\r\n",
    "\r\n",
    "# Set seed for experiment reproducibility\r\n",
    "seed = 2021\r\n",
    "#env.seed(seed)\r\n",
    "random.seed(seed)\r\n",
    "#tf.random.set_seed(seed)\r\n",
    "#np.random.seed(seed)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "observation0 = [{'remainingOverageTime': 60,\r\n",
    " 'step': 0,\r\n",
    " 'agentIndex': 0,\r\n",
    " 'reward': 0,\r\n",
    " 'lastActions': []}]\r\n",
    "configuration0 = {'episodeSteps': 2000, 'actTimeout': 0.25, 'runTimeout': 1200, 'banditCount': 5, 'decayRate': 0.97, 'sampleResolution': 100}\r\n",
    "\r\n",
    "mab_player = MABanditPlayer(observation0, configuration0)\r\n",
    "\r\n",
    "def keras_agent(observation, configuration):\r\n",
    "    #print(observation)\r\n",
    "    #print(configuration)\r\n",
    "    \r\n",
    "    global mab_player\r\n",
    "    bandit=0\r\n",
    "    bandit=(mab_player.play(observation, configuration)).item()  \r\n",
    "    return int(bandit)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting submission.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "env_test = make(\"mab\", debug=True)\r\n",
    "\r\n",
    "steps = env_test.run([\"submission.py\", \"agent_random_.py\"])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "env_test.reset()\r\n",
    "env_test.run([\"submission.py\", \"agent_random_.py\"])\r\n",
    "env_test.render(mode='ipython', width=800, height=700)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-5c6f98f61f6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0menv_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"submission.py\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"agent_random_.py\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0menv_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ipython'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m700\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\kaggle_environments\\core.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, agents)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunTimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m             \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\kaggle_environments\\core.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(none_action)\u001b[0m\n\u001b[0;32m    643\u001b[0m                 \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact_agent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m                 \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact_agent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m             \u001b[1;31m# results is a list of tuples where the first element is an agent action and the second is the agent log\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\kaggle_environments\\core.py\u001b[0m in \u001b[0;36mact_agent\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnone_action\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"observation\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\kaggle_environments\\agent.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[0mtraceback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_exc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merr_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\kaggle_environments\\agent.py\u001b[0m in \u001b[0;36mcallable_agent\u001b[1;34m(observation, configuration)\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__code__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mco_argcount\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[1;32melse\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\산학프로젝트\\구글\\submission.py\u001b[0m in \u001b[0;36mkeras_agent\u001b[1;34m(observation, configuration)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[0mbandit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m     \u001b[0mbandit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmab_player\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfiguration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbandit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\산학프로젝트\\구글\\submission.py\u001b[0m in \u001b[0;36mplay\u001b[1;34m(self, observation, configuration)\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[1;31m#bandit = model_bandit.predict_classes(state) #state = tf.expand_dims(state, 0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[0mbandit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, target, graph, config)\u001b[0m\n\u001b[0;32m   1594\u001b[0m           \u001b[0mprotocol\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mconfiguration\u001b[0m \u001b[0moptions\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1595\u001b[0m     \"\"\"\n\u001b[1;32m-> 1596\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1597\u001b[0m     \u001b[1;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1598\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, target, graph, config)\u001b[0m\n\u001b[0;32m    709\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m       \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 711\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_NewSessionRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    712\u001b[0m       \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<__main__.ActorCritic at 0x1a101315b50>"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}