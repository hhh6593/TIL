{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import collections\r\n",
    "import gym\r\n",
    "# !pip install tensorflow==1.15.0\r\n",
    "import tensorflow as tf\r\n",
    "tf.__version__\r\n",
    "import tqdm\r\n",
    "\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "from tensorflow.keras import layers\r\n",
    "from typing import Any, List, Sequence, Tuple\r\n",
    "\r\n",
    "from kaggle_environments import make, evaluate\r\n",
    "\r\n",
    "from gym import spaces\r\n",
    "\r\n",
    "import random"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading environment football failed: No module named 'gfootball'\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# df = pd.read_csv('age_range_video.csv')\r\n",
    "# df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# df.sort_values('View rate', inplace=True)\r\n",
    "# df.reset_index(inplace=True, drop=True)\r\n",
    "# df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "int(1) in range(0,100)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "class MABanditGym:   \r\n",
    "    \r\n",
    "    \r\n",
    "    def __init__(self, agent2=\"random\"):\r\n",
    "        ks_env = make(\"mab\", debug=True)\r\n",
    "        self.env = ks_env.train([None, agent2])\r\n",
    "        self.nrounds = 2000\r\n",
    "        self.banditCount = ks_env.configuration.banditCount\r\n",
    "        self.prev_reward = 0\r\n",
    "        \r\n",
    "        # space 개념 : http://gym.openai.com/docs/#spaces\r\n",
    "        self.action_space = spaces.Discrete(self.banditCount)\r\n",
    "        low = -np.ones((self.nrounds,), dtype=np.float32)\r\n",
    "        high = -low*(self.banditCount-1)\r\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\r\n",
    "        # 최소 및 최대에 해당하는 reward 튜플\r\n",
    "        self.reward_range = (-10,1) # 최소 -10, 최대 1\r\n",
    "        self.grid = -np.ones((self.nrounds,2)) #2000,2 격자 state\r\n",
    "        self.obs=np.array(self.grid).reshape(self.nrounds*2) #(4000,) 배열 생성\r\n",
    "        # 안정된 기준선이 정의되지 않은 경우 오류 발생\r\n",
    "        self.spec = None\r\n",
    "        self.metadata = None\r\n",
    "    def reset(self):\r\n",
    "        #print(env.obs)\r\n",
    "        self.env.reset()\r\n",
    "        self.grid = -np.ones((self.nrounds,2))\r\n",
    "        self.obs=np.array(self.grid).reshape(self.nrounds*2)\r\n",
    "        self.prev_reward = 0\r\n",
    "        return self.obs\r\n",
    "    def change_reward(self, old_reward, done):\r\n",
    "        if old_reward == 1000: # 에이전트가 이긴 경우\r\n",
    "            return 0\r\n",
    "        elif done: # 상대가 이긴 경우\r\n",
    "            return -10\r\n",
    "        else: # Reward 1/2000 \r\n",
    "            return old_reward\r\n",
    "    def step(self, action):\r\n",
    "        _={}\r\n",
    "        # Agent가 유효한 Action을 하는지 검증\r\n",
    "        is_valid = (int(action) in range(0,self.banditCount)) #Bandit의 범위 안에 들어가는 Action일 경우 True 반환\r\n",
    "        #valid_moves = [bnd for bnd in range(config.banditCount)]\r\n",
    "       \r\n",
    "        if is_valid: # True를 반환할 경우 Action 실행\r\n",
    "            current_obs = self.env.step(int(action))\r\n",
    "            \r\n",
    "            for pos in range(0,2):\r\n",
    "                #print(current_obs)\r\n",
    "                self.grid[current_obs[0]['step']-1][pos]=current_obs[0]['lastActions'][pos]\r\n",
    "            self.obs=np.array(self.grid).reshape(self.nrounds*2)\r\n",
    "            old_reward= current_obs[0]['reward']\r\n",
    "            done = (current_obs[0]['step']==self.nrounds-1 and current_obs[0]['reward']<600) #에피소드 끝나는 지점 설정\r\n",
    "            reward = old_reward- self.prev_reward  #self.change_reward(old_reward, done)\r\n",
    "            self.prev_reward=old_reward\r\n",
    "        else: # False일 경우 게임 종료 후 Agent에게 벌칙 적용\r\n",
    "            reward, done, _ = -10, True, {}\r\n",
    "        #print(self.obs, reward, done, _  )    \r\n",
    "        return self.obs, reward, done, _    \r\n",
    "\r\n",
    "\r\n",
    "    \r\n",
    "    def seed(self, seed=None): #시드값 고정\r\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\r\n",
    "        return [seed]\r\n",
    "\r\n",
    "# Create the environment for training tasks\r\n",
    "env = MABanditGym(agent2=\"random\")\r\n",
    "# Set seed for experiment reproducibility\r\n",
    "seed = 42\r\n",
    "env.seed(seed)\r\n",
    "random.seed(seed)\r\n",
    "tf.random.set_seed(seed)\r\n",
    "np.random.seed(seed)\r\n",
    "\r\n",
    "# Small epsilon value for stabilizing division operations\r\n",
    "eps = np.finfo(np.float32).eps.item()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "#모델 구축\r\n",
    "class ActorCritic(tf.keras.Model):\r\n",
    "  \"\"\"Combined actor-critic network.\"\"\"\r\n",
    "\r\n",
    "  def __init__(\r\n",
    "      self, \r\n",
    "      num_actions: int, \r\n",
    "      num_hidden_units: int):\r\n",
    "    \"\"\"Initialize.\"\"\"\r\n",
    "    super().__init__()\r\n",
    "    self.common = layers.Dense(num_hidden_units, activation=\"relu\")\r\n",
    "    self.actor = layers.Dense(num_actions)\r\n",
    "    self.critic = layers.Dense(1)\r\n",
    "\r\n",
    "  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\r\n",
    "    x = self.common(inputs)\r\n",
    "    return self.actor(x), self.critic(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "num_actions = env.action_space.n  # 100\r\n",
    "print(num_actions)\r\n",
    "num_hidden_units = 128\r\n",
    "\r\n",
    "model = ActorCritic(num_actions, num_hidden_units)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "random.betavariate(1, 5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4088948191470329"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\r\n",
    " \r\n",
    "1. 환경에서 에이전트를 실행하여 에피소드당 training 데이터를 수집합니다.\r\n",
    "2. 각 단계에서 예측 return값을 계산합니다.\r\n",
    "3. 모델 loss 계산\r\n",
    "4. 역전파\r\n",
    "5. 마지막 에피소드까지 1~4과정 반복"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "# Wrap OpenAI Gym's `env.step` call as an operation in a TensorFlow function.\r\n",
    "# This would allow it to be included in a callable TensorFlow graph.\r\n",
    "\r\n",
    "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\r\n",
    "  \"\"\"Returns state, reward and done flag given an action.\"\"\"\r\n",
    "\r\n",
    "  state, reward, done, _ = env.step(action)\r\n",
    "  return (state.astype(np.float32), \r\n",
    "          np.array(reward, np.int32), \r\n",
    "          np.array(done, np.int32))\r\n",
    "\r\n",
    "\r\n",
    "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\r\n",
    "  return tf.numpy_function(env_step, [action], \r\n",
    "                           [tf.float32, tf.int32, tf.int32])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "tf.expand_dims([1,0], 0)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1, 0]])>"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "def run_episode(\r\n",
    "    initial_state: tf.Tensor,  \r\n",
    "    model: tf.keras.Model, \r\n",
    "    max_steps: int) -> List[tf.Tensor]:\r\n",
    "  \"\"\"Runs a single episode to collect training data.\"\"\"\r\n",
    "\r\n",
    "  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\r\n",
    "  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\r\n",
    "  rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\r\n",
    "\r\n",
    "  initial_state_shape = initial_state.shape\r\n",
    "  state = initial_state\r\n",
    "\r\n",
    "  for t in tf.range(max_steps):\r\n",
    "    # State를 Tensor 형태로 변환\r\n",
    "    state = tf.expand_dims(state, 0)\r\n",
    "\r\n",
    "    # 모델을 돌리고 Aciton 확률값과 Critic 값을 추출\r\n",
    "    action_logits_t, value = model(state)\r\n",
    "\r\n",
    "    # 다음 Sample Action에 Action 확률 적용\r\n",
    "    action = tf.random.categorical(action_logits_t, 1)[0, 0]\r\n",
    "    action_probs_t = tf.nn.softmax(action_logits_t)\r\n",
    "\r\n",
    "    # Store critic values\r\n",
    "    values = values.write(t, tf.squeeze(value))\r\n",
    "\r\n",
    "    # 선택한 작업?의 로그 확률 저장\r\n",
    "    action_probs = action_probs.write(t, action_probs_t[0, action])\r\n",
    "\r\n",
    "    # 다음 State에 보상을 얻기 위한 Action 적용\r\n",
    "    state, reward, done = tf_env_step(action)\r\n",
    "    print(reward)\r\n",
    "    state.set_shape(initial_state_shape)\r\n",
    "\r\n",
    "     # Reward 저장\r\n",
    "    rewards = rewards.write(t, reward)\r\n",
    "\r\n",
    "    if tf.cast(done, tf.bool):\r\n",
    "      break\r\n",
    "\r\n",
    "  action_probs = action_probs.stack()\r\n",
    "  values = values.stack()\r\n",
    "  rewards = rewards.stack()\r\n",
    "\r\n",
    "  return action_probs, values, rewards"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "def get_expected_return(\r\n",
    "    rewards: tf.Tensor, \r\n",
    "    gamma: float, \r\n",
    "    standardize: bool = True) -> tf.Tensor:\r\n",
    "  \"\"\"Compute expected returns per timestep.\"\"\"\r\n",
    "\r\n",
    "  n = tf.shape(rewards)[0]\r\n",
    "  returns = tf.TensorArray(dtype=tf.float32, size=n)\r\n",
    "\r\n",
    "  # Rewards의 끝에서 부터 reward의 합계 저장 후 Returns에 저장\r\n",
    "  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\r\n",
    "  discounted_sum = tf.constant(0.0)\r\n",
    "  discounted_sum_shape = discounted_sum.shape\r\n",
    "  for i in tf.range(n):\r\n",
    "    reward = rewards[i]\r\n",
    "    discounted_sum = reward + gamma * discounted_sum\r\n",
    "    discounted_sum.set_shape(discounted_sum_shape)\r\n",
    "    returns = returns.write(i, discounted_sum)\r\n",
    "  returns = returns.stack()[::-1]\r\n",
    "\r\n",
    "  if standardize:\r\n",
    "    returns = ((returns - tf.math.reduce_mean(returns)) / \r\n",
    "               (tf.math.reduce_std(returns) + eps))\r\n",
    "\r\n",
    "  return returns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\r\n",
    "\r\n",
    "def compute_loss(\r\n",
    "    action_probs: tf.Tensor,  \r\n",
    "    values: tf.Tensor,  \r\n",
    "    returns: tf.Tensor) -> tf.Tensor:\r\n",
    "  \"\"\"Computes the combined actor-critic loss.\"\"\"\r\n",
    "\r\n",
    "  advantage = returns - values\r\n",
    "\r\n",
    "  action_log_probs = tf.math.log(action_probs)\r\n",
    "  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\r\n",
    "\r\n",
    "  critic_loss = huber_loss(values, returns)\r\n",
    "\r\n",
    "  return actor_loss + critic_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\r\n",
    "\r\n",
    "\r\n",
    "@tf.function\r\n",
    "def train_step(\r\n",
    "    initial_state: tf.Tensor, \r\n",
    "    model: tf.keras.Model, \r\n",
    "    optimizer: tf.keras.optimizers.Optimizer, \r\n",
    "    gamma: float, \r\n",
    "    max_steps_per_episode: int) -> tf.Tensor:\r\n",
    "  \"\"\"Runs a model training step.\"\"\"\r\n",
    "  #print(\"Hello\")\r\n",
    "  with tf.GradientTape() as tape:\r\n",
    "\r\n",
    "    # Run the model for one episode to collect training data\r\n",
    "    action_probs, values, rewards = run_episode(\r\n",
    "        initial_state, model, max_steps_per_episode) \r\n",
    "\r\n",
    "    # Calculate expected returns\r\n",
    "    returns = get_expected_return(rewards, gamma)\r\n",
    "\r\n",
    "    # Convert training data to appropriate TF tensor shapes\r\n",
    "    action_probs, values, returns = [\r\n",
    "        tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \r\n",
    "\r\n",
    "    # Calculating loss values to update our network\r\n",
    "    loss = compute_loss(action_probs, values, returns)\r\n",
    "\r\n",
    "  # Compute the gradients from the loss\r\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\r\n",
    "\r\n",
    "  # Apply the gradients to the model's parameters\r\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n",
    "\r\n",
    "  episode_reward = tf.math.reduce_sum(rewards)\r\n",
    "\r\n",
    "  return episode_reward"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "%%time\r\n",
    "\r\n",
    "max_episodes = 1\r\n",
    "max_steps_per_episode = 2000\r\n",
    "\r\n",
    "# 평균 reward가 600/100 이상이면 문제가 해결된 것으로 간주\r\n",
    "# consecutive trials\r\n",
    "reward_threshold = 600\r\n",
    "running_reward = 0\r\n",
    "\r\n",
    "# 감마(감가율) 설정\r\n",
    "gamma = 0.99\r\n",
    "\r\n",
    "with tqdm.trange(max_episodes) as t:\r\n",
    "  for i in t:\r\n",
    "    new_env=env.reset()\r\n",
    "    #print(new_env)\r\n",
    "    #print(type(new_env))\r\n",
    "    #print(new_env.shape)\r\n",
    "    \r\n",
    "    initial_state = tf.constant(new_env, dtype=tf.float32)\r\n",
    "    episode_reward = int(train_step(\r\n",
    "        initial_state, model, optimizer, gamma, max_steps_per_episode))\r\n",
    "    #print('episode_reward: ', episode_reward)\r\n",
    "    running_reward = episode_reward*0.01 + running_reward*.99\r\n",
    "\r\n",
    "    t.set_description(f'Episode {i}')\r\n",
    "    t.set_postfix(\r\n",
    "        episode_reward=episode_reward, running_reward=running_reward)\r\n",
    "\r\n",
    "    # 에피소드 5회 당 평균 reward 확인\r\n",
    "    if i % 5 == 0:\r\n",
    "      print(f'Episode {i}: average reward: {running_reward}')\r\n",
    "\r\n",
    "    if running_reward > reward_threshold:  \r\n",
    "        break\r\n",
    "\r\n",
    "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensor(\"while/PyFunc:1\", dtype=int32, device=/job:localhost/replica:0/task:0)\n",
      "Tensor(\"while/PyFunc:1\", dtype=int32, device=/job:localhost/replica:0/task:0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Episode 0: 100%|██████████| 1/1 [00:15<00:00, 15.19s/it, episode_reward=184, running_reward=1.84]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode 0: average reward: 1.84\n",
      "\n",
      "Solved at episode 0: average reward: 1.84!\n",
      "Wall time: 15.2 s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "filename = 'bandit_model'\r\n",
    "model.save(filename) # creates a HDF5 file 'bandit_model.h5'\r\n",
    "#del model  # deletes the existing model"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: bandit_model\\assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "from tensorflow.keras.models import load_model\r\n",
    "# returns a compiled model\r\n",
    "# identical to the previous one\r\n",
    "model_bandit = load_model('bandit_model')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "%%writefile agent_random_.py\r\n",
    "import random\r\n",
    "def agent_random_(obs, config):\r\n",
    "    #print(obs)\r\n",
    "    #print(config)\r\n",
    "    valid_moves = [bnd for bnd in range(config['banditCount'])]\r\n",
    "    return random.choice(valid_moves)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Writing agent_random_.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "%%writefile submission.py\r\n",
    "\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "\r\n",
    "from tensorflow.keras.models import load_model\r\n",
    "# returns a compiled model\r\n",
    "# identical to the previous one\r\n",
    "model_bandit = load_model('bandit_model')\r\n",
    "\r\n",
    "\r\n",
    "class MABanditPlayer:\r\n",
    "    global model_bandit\r\n",
    "    \r\n",
    "    \r\n",
    "    def __init__(self, observation, configuration):\r\n",
    "        \r\n",
    "        self.nrounds = configuration['episodeSteps']\r\n",
    "        self.banditCount = configuration['banditCount']\r\n",
    "        self.prev_reward = 0\r\n",
    "        self.grid = -np.ones((self.nrounds,2))\r\n",
    "        self.obs=np.array(self.grid).reshape(self.nrounds*2)\r\n",
    "        self.prev_reward = 0\r\n",
    "        \r\n",
    "    def reset(self):\r\n",
    "        #print(env.obs)\r\n",
    "        self.env.reset()\r\n",
    "        self.grid = -np.ones((self.nrounds,2))\r\n",
    "        self.obs=np.array(self.grid).reshape(self.nrounds*2)\r\n",
    "        self.prev_reward = 0\r\n",
    "        return self.obs\r\n",
    "    \r\n",
    "    def play(self, observation, configuration):\r\n",
    "        bandit=0\r\n",
    "        if observation['step']>0:\r\n",
    "            \r\n",
    "        \r\n",
    "            for pos in range(0,2):\r\n",
    "                    #print(current_obs)\r\n",
    "                    self.grid[observation['step']-1][pos]=observation['lastActions'][pos]\r\n",
    "            new_reward= observation['reward']\r\n",
    "            reward = new_reward- self.prev_reward\r\n",
    "            self.prev_reward=new_reward\r\n",
    "            self.obs=np.array(self.grid).reshape(self.nrounds*2)\r\n",
    "            \r\n",
    "            # Convert state into a batched tensor (batch size = 1)\r\n",
    "            state = tf.expand_dims(self.obs,0)\r\n",
    "            # Run the model and to get action probabilities and critic value\r\n",
    "            action_logits_t, value = model_bandit(state)\r\n",
    "            # Sample next action from the action probability distribution\r\n",
    "            action = tf.random.categorical(action_logits_t, 1)[0, 0]\r\n",
    "            #bandit = model_bandit.predict_classes(state) #state = tf.expand_dims(state, 0)\r\n",
    "            with tf.compat.v1.Session() as sess:\r\n",
    "                bandit = action.numpy()\r\n",
    "            \r\n",
    "        else:\r\n",
    "            valid_moves = [bnd for bnd in range(configuration['banditCount'])]\r\n",
    "            bandit = np.dtype('int32').type(random.choice(valid_moves))\r\n",
    "\r\n",
    "        return(bandit)    \r\n",
    "    def seed(self, seed=None):\r\n",
    "        self.np_random, seed = seeding.np_random(seed)\r\n",
    "        return [seed]\r\n",
    "# Create the environment\r\n",
    "\r\n",
    "\r\n",
    "# Set seed for experiment reproducibility\r\n",
    "seed = 2021\r\n",
    "#env.seed(seed)\r\n",
    "random.seed(seed)\r\n",
    "#tf.random.set_seed(seed)\r\n",
    "#np.random.seed(seed)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "observation0 = [{'remainingOverageTime': 60,\r\n",
    " 'step': 0,\r\n",
    " 'agentIndex': 0,\r\n",
    " 'reward': 0,\r\n",
    " 'lastActions': []}]\r\n",
    "configuration0 = {'episodeSteps': 2000, 'actTimeout': 0.25, 'runTimeout': 1200, 'banditCount': 5, 'decayRate': 0.97, 'sampleResolution': 100}\r\n",
    "\r\n",
    "mab_player = MABanditPlayer(observation0, configuration0)\r\n",
    "\r\n",
    "def keras_agent(observation, configuration):\r\n",
    "    #print(observation)\r\n",
    "    #print(configuration)\r\n",
    "    \r\n",
    "    global mab_player\r\n",
    "    bandit=0\r\n",
    "    bandit=(mab_player.play(observation, configuration)).item()  \r\n",
    "    return int(bandit)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Writing submission.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "env_test = make(\"mab\", debug=True)\r\n",
    "\r\n",
    "steps = env_test.run([\"submission.py\", \"agent_random_.py\"])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "env_test.reset()\r\n",
    "env_test.run([\"submission.py\", \"agent_random_.py\"])\r\n",
    "env_test.render(mode='ipython', width=800, height=700)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-5c6f98f61f6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0menv_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"submission.py\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"agent_random_.py\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0menv_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ipython'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m700\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\kaggle_environments\\core.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, agents)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunTimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m             \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\kaggle_environments\\core.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(none_action)\u001b[0m\n\u001b[0;32m    643\u001b[0m                 \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact_agent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m                 \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact_agent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m             \u001b[1;31m# results is a list of tuples where the first element is an agent action and the second is the agent log\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\kaggle_environments\\core.py\u001b[0m in \u001b[0;36mact_agent\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnone_action\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"observation\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\kaggle_environments\\agent.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[0mtraceback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_exc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merr_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\kaggle_environments\\agent.py\u001b[0m in \u001b[0;36mcallable_agent\u001b[1;34m(observation, configuration)\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__code__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mco_argcount\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[1;32melse\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\황한희\\Desktop\\산학프로젝트\\google\\submission.py\u001b[0m in \u001b[0;36mkeras_agent\u001b[1;34m(observation, configuration)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[0mbandit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m     \u001b[0mbandit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmab_player\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfiguration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbandit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\황한희\\Desktop\\산학프로젝트\\google\\submission.py\u001b[0m in \u001b[0;36mplay\u001b[1;34m(self, observation, configuration)\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[1;31m# Sample next action from the action probability distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_logits_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[1;31m#bandit = model_bandit.predict_classes(state) #state = tf.expand_dims(state, 0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\random_ops.py\u001b[0m in \u001b[0;36mcategorical\u001b[1;34m(logits, num_samples, dtype, seed, name)\u001b[0m\n\u001b[0;32m    523\u001b[0m   \"\"\"\n\u001b[0;32m    524\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"categorical\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmultinomial_categorical_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\random_ops.py\u001b[0m in \u001b[0;36mmultinomial_categorical_impl\u001b[1;34m(logits, num_samples, dtype, seed)\u001b[0m\n\u001b[0;32m    530\u001b[0m   \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"logits\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m   \u001b[0mseed1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m   return gen_random_ops.multinomial(\n\u001b[0m\u001b[0;32m    533\u001b[0m       logits, num_samples, seed=seed1, seed2=seed2, output_dtype=dtype)\n\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\gen_random_ops.py\u001b[0m in \u001b[0;36mmultinomial\u001b[1;34m(logits, num_samples, seed, seed2, output_dtype, name)\u001b[0m\n\u001b[0;32m     45\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Multinomial\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"seed\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"seed2\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         seed2, \"output_dtype\", output_dtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<__main__.ActorCritic at 0x1a101315b50>"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}